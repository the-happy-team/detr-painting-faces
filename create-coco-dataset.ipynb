{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create COCOrdiais dataset from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cocordiais_utils as cocordiais\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image as PImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET = \"./data\"\n",
    "PATH_COCORDIAIS = \"./cocordiais\"\n",
    "\n",
    "IMAGES_METFACES = os.path.join(PATH_DATASET, \"metfaces-source\")\n",
    "IMAGES_CORDIAIS = os.path.join(PATH_DATASET, \"cordiais-source\")\n",
    "\n",
    "FILES_METFACES = sorted([f.replace(\".jpg\", \"\") for f in os.listdir(IMAGES_METFACES) if f.endswith(\".jpg\")])\n",
    "FILES_CORDIAIS = sorted([f.replace(\".jpg\", \"\") for f in os.listdir(IMAGES_CORDIAIS) if f.endswith(\".jpg\")])\n",
    "\n",
    "random.seed(101010)\n",
    "shuffled_cordiais = random.sample(FILES_CORDIAIS, k=len(FILES_CORDIAIS))\n",
    "shuffled_metfaces = random.sample(FILES_METFACES, k=len(FILES_METFACES))\n",
    "\n",
    "DATA_INFO = [\n",
    "  {\n",
    "    \"name\": \"cordiais\",\n",
    "    \"license_id\": 1,\n",
    "    \"date\": \"2023-06-10 00:00:00\",\n",
    "    \"splits\": {\n",
    "      \"test\": shuffled_cordiais[int(0.5 * len(FILES_CORDIAIS)):],\n",
    "      \"train\": shuffled_cordiais[int(0.1 * len(FILES_CORDIAIS)) : int(0.5 * len(FILES_CORDIAIS))],\n",
    "      \"validation\": shuffled_cordiais[:int(0.1 * len(FILES_CORDIAIS))]\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"metfaces\",\n",
    "    \"license_id\": 1,\n",
    "    \"date\": \"2020-06-10 00:00:00\",\n",
    "    \"splits\": {\n",
    "      \"test\": [],\n",
    "      \"train\": shuffled_metfaces[int(0.2 * len(FILES_METFACES)):],\n",
    "      \"validation\": shuffled_metfaces[:int(0.2 * len(FILES_METFACES))]\n",
    "    }\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy files to cocordiais directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in DATA_INFO:\n",
    "  for split in dset[\"splits\"].keys():\n",
    "    os.makedirs(os.path.join(PATH_COCORDIAIS, split), exist_ok=True)\n",
    "    for fn in dset[\"splits\"][split]:\n",
    "      shutil.copy2(\n",
    "        os.path.join(PATH_DATASET, \"%s-source\" % dset[\"name\"], \"%s.jpg\" % fn),\n",
    "        os.path.join(PATH_COCORDIAIS, split)\n",
    "      )\n",
    "\n",
    "shutil.copy2(\n",
    "  os.path.join(PATH_DATASET, \"id2label.json\"),\n",
    "  os.path.join(PATH_COCORDIAIS)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this could be a class... maybe ?\n",
    "files = {}\n",
    "img2id = {}\n",
    "id2img = {}\n",
    "coco_annotations = {}\n",
    "\n",
    "for s in [\"test\", \"train\", \"validation\"]:\n",
    "  split_dir = os.path.join(PATH_COCORDIAIS, s)\n",
    "  split_file_list = sorted([f.replace(\".jpg\", \"\") for f in os.listdir(split_dir) if f.endswith(\".jpg\")])\n",
    "  files[s] = set(split_file_list)\n",
    "  img2id[s] = {f: i for i,f in enumerate(split_file_list)}\n",
    "  id2img[s] = {}\n",
    "  coco_annotations[s] = []\n",
    "\n",
    "\n",
    "def get_split(fn):\n",
    "  for s in [\"test\", \"train\", \"validation\"]:\n",
    "    if fn in files[s]:\n",
    "      return s\n",
    "\n",
    "\n",
    "for dset in DATA_INFO:\n",
    "  dset_name = dset[\"name\"]\n",
    "  dataset_path_in = os.path.join(PATH_DATASET, \"%s.json\" % dset_name)\n",
    "\n",
    "  with open(dataset_path_in) as json_file_in:\n",
    "    data = json.load(json_file_in)\n",
    "    for obj in data:\n",
    "      my_split = get_split(obj[\"source_image\"])\n",
    "      my_img_id = img2id[my_split][obj[\"source_image\"]]\n",
    "\n",
    "      id2img[my_split][my_img_id] = {\n",
    "        \"id\": my_img_id,\n",
    "        \"width\": obj[\"source_image_w\"],\n",
    "        \"height\": obj[\"source_image_h\"],\n",
    "        \"file_name\": \"%s.jpg\" % obj[\"source_image\"],\n",
    "        \"license\": dset[\"license_id\"],\n",
    "        \"date_captured\": dset[\"date\"],\n",
    "        \"coco_url\": \"\",\n",
    "        \"flickr_url\": \"\"\n",
    "      }\n",
    "\n",
    "      poly = [\n",
    "        [obj[\"face_rect_xyxy\"][0], obj[\"face_rect_xyxy\"][1]],\n",
    "        [obj[\"face_rect_xyxy\"][2], obj[\"face_rect_xyxy\"][1]],\n",
    "        [obj[\"face_rect_xyxy\"][2], obj[\"face_rect_xyxy\"][3]],\n",
    "        [obj[\"face_rect_xyxy\"][0], obj[\"face_rect_xyxy\"][3]]\n",
    "      ]\n",
    "\n",
    "      ann_obj = {\n",
    "        \"id\": len(coco_annotations[my_split]),\n",
    "        \"image_id\": my_img_id,\n",
    "        \"category_id\": cocordiais.LABEL2ID[obj[\"gender\"]],\n",
    "        \"segmentation\": list([poly]),\n",
    "        \"area\": obj[\"face_rect_xywh\"][2] * obj[\"face_rect_xywh\"][3],\n",
    "        \"bbox\": obj[\"face_rect_xywh\"],\n",
    "        \"iscrowd\": 0,\n",
    "      }\n",
    "\n",
    "      coco_annotations[my_split].append(ann_obj)\n",
    "\n",
    "for s in [\"test\", \"train\", \"validation\"]:\n",
    "  coco_split = cocordiais.get_cocordiais_info()\n",
    "  coco_split[\"images\"] = list(id2img[s].values())\n",
    "  coco_split[\"annotations\"] = list(coco_annotations[s])\n",
    "\n",
    "  out_file = os.path.join(PATH_COCORDIAIS, s, \"cocordiais.json\")\n",
    "\n",
    "  with open(out_file, 'w') as json_file_out_write:\n",
    "    json.dump(coco_split, json_file_out_write)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create COCOrdiais from hf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import datetime\n",
    "import huggingface_hub\n",
    "import json\n",
    "import os\n",
    "from PIL import Image as PImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_ = datasets.load_dataset(\"thiagohersan/cordiais-faces\")\n",
    "hf_dataset = hf_dataset_[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=1010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCORDIAIS_DATA_PATH = \"./cocordiais-from-hf\"\n",
    "\n",
    "for split_name, objs in hf_dataset.items():\n",
    "  split_data_path = os.path.join(COCORDIAIS_DATA_PATH, split_name)\n",
    "  os.makedirs(split_data_path)\n",
    "\n",
    "  json_path_out = os.path.join(split_data_path, \"cocordiais.json\")\n",
    "  cocordiais_obj = cocordiais.get_cocordiais_info()\n",
    "\n",
    "  for obj in objs:\n",
    "    img_path = os.path.join(split_data_path, obj[\"image_filename\"])\n",
    "\n",
    "    if not os.path.isfile(img_path):\n",
    "      obj[\"image\"].save(img_path, \"JPEG\")\n",
    "\n",
    "    cocordiais_obj[\"images\"].append({\n",
    "      \"id\": obj[\"image_id\"],\n",
    "      \"width\": obj[\"width\"],\n",
    "      \"height\": obj[\"height\"],\n",
    "      \"file_name\": obj[\"image_filename\"],\n",
    "      \"license\": obj[\"license_id\"],\n",
    "      \"date_captured\": obj[\"date_captured\"],\n",
    "      \"coco_url\": \"\",\n",
    "      \"flickr_url\": \"\"\n",
    "    })\n",
    "\n",
    "    ann_objs = obj[\"objects\"]\n",
    "    for ann_idx in range(len(ann_objs[\"bbox_id\"])):\n",
    "      cocordiais_obj[\"annotations\"].append({\n",
    "        \"id\": ann_objs[\"bbox_id\"][ann_idx],\n",
    "        \"image_id\": obj[\"image_id\"],\n",
    "        \"category_id\": ann_objs[\"category\"][ann_idx],\n",
    "        \"area\": ann_objs[\"area\"][ann_idx],\n",
    "        \"bbox\": ann_objs[\"bbox\"][ann_idx],\n",
    "        \"iscrowd\": 1 if ann_objs[\"is_crowd\"][ann_idx] else 0\n",
    "      })\n",
    "\n",
    "  with open(json_path_out, 'w') as json_file_out_write:\n",
    "    json.dump(cocordiais_obj, json_file_out_write)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check jsons, images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import DetrImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "  def __init__(self, img_folder, processor, train=True):\n",
    "    ann_file = os.path.join(img_folder, \"cocordiais.json\")\n",
    "    super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "    self.processor = processor\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # read in PIL image and target in COCO format\n",
    "    # feel free to add data augmentation here before passing them to the next step\n",
    "    img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "    \n",
    "    # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "    image_id = self.ids[idx]\n",
    "    target = {'image_id': image_id, 'annotations': target}\n",
    "    encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "    pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "    target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "    return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCORDIAIS_DATA_PATH = \"./cocordiais-hf\"\n",
    "COCORDIAIS_DATA_PATH = \"./cocordiais\"\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=os.path.join(COCORDIAIS_DATA_PATH, 'train'), processor=processor)\n",
    "test_dataset = CocoDetection(img_folder=os.path.join(COCORDIAIS_DATA_PATH, 'test'), processor=processor, train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = train_dataset.coco.getImgIds()\n",
    "image_id = random.sample(image_ids, k=1)[0]\n",
    "\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(COCORDIAIS_DATA_PATH, 'train', image['file_name']))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "  label = id2label[annotation['category_id']]\n",
    "  x,y,w,h = tuple(annotation['bbox'])\n",
    "  draw.rectangle((x,y,x+w,y+h), outline='red', width=2)\n",
    "  draw.text((x, y), label, fill='white')\n",
    "\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('hf-model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

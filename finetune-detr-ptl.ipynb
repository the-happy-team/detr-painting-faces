{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune DETR to detect female-ish faces in paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade scipy transformers datasets huggingface_hub pytorch-lightning pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DetrConfig, DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "from Cocordiais import CocordiaisData\n",
    "from PIL import Image as PImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from HF and turn to COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETR_MODEL = \"facebook/detr-resnet-50\"\n",
    "HF_DATASET = \"thiagohersan/cordiais-faces\"\n",
    "HF_MODEL= \"thiagohersan/detr-cordiais\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr_processor = DetrImageProcessor.from_pretrained(\n",
    "  DETR_MODEL,\n",
    "  size={\n",
    "    \"shortest_edge\": 800,\n",
    "    \"longest_edge\": 800\n",
    "  }\n",
    ")\n",
    "\n",
    "cocordiais_data = CocordiaisData(detr_processor)\n",
    "\n",
    "hf_dataset = datasets.load_dataset(HF_DATASET)\n",
    "hf_dataset = hf_dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True, seed=101010)\n",
    "\n",
    "train_dataset = hf_dataset[\"train\"].with_transform(cocordiais_data.to_coco(train=True))\n",
    "test_dataset = hf_dataset[\"test\"].with_transform(cocordiais_data.to_coco(train=False))\n",
    "\n",
    "print(\"Number of examples:\\n  Train: %s\\n  Test: %s\" % (len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "  train_dataset,\n",
    "  collate_fn=cocordiais_data.collate_batch,\n",
    "  batch_size=12,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "  test_dataset,\n",
    "  collate_fn=cocordiais_data.collate_batch,\n",
    "  batch_size=4,\n",
    "  shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "pixel_values, _, target = train_dataset[0].values()\n",
    "print(pixel_values.shape)\n",
    "print(target)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "pimg = T.ToPILImage()(batch[\"pixel_values\"][0])\n",
    "print(pimg.size)\n",
    "pimg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with PyTorchLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_dataset.features[\"objects\"].feature[\"category\"].names\n",
    "id2label = {i:l for i,l in enumerate(labels)}\n",
    "label2id = {l:i for i,l in id2label.items()}\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "  def __init__(self, dl_train, dl_val, lr, lr_backbone, weight_decay):\n",
    "    super().__init__()\n",
    "    # replace COCO classification head with custom head\n",
    "    self.model = DetrForObjectDetection.from_pretrained(\n",
    "      DETR_MODEL,\n",
    "      revision=\"no_timm\", \n",
    "      num_labels=len(id2label),\n",
    "      num_queries=16,\n",
    "      ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    self.lr = lr\n",
    "    self.lr_backbone = lr_backbone\n",
    "    self.weight_decay = weight_decay\n",
    "\n",
    "    self.dataloader_train = dl_train\n",
    "    self.dataloader_val = dl_val\n",
    "    self.batch_size_train = dl_train.batch_size\n",
    "    self.batch_size_val = dl_val.batch_size\n",
    "\n",
    "  def forward(self, pixel_values, pixel_mask):\n",
    "    outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "    return outputs\n",
    "\n",
    "  def common_step(self, batch, batch_idx):\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_mask = batch[\"pixel_mask\"]\n",
    "    labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "    outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss_dict = outputs.loss_dict\n",
    "\n",
    "    return loss, loss_dict\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "    self.log(\"training_loss\", loss, batch_size=self.batch_size_train)\n",
    "    for k,v in loss_dict.items():\n",
    "      self.log(\"train_\" + k, v.item(), batch_size=self.batch_size_train)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "    self.log(\"validation_loss\", loss, batch_size=self.batch_size_val)\n",
    "    for k,v in loss_dict.items():\n",
    "      self.log(\"validation_\" + k, v.item(), batch_size=self.batch_size_val)\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    param_dicts = [\n",
    "      {\n",
    "        \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]\n",
    "      },\n",
    "      {\n",
    "        \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": self.lr_backbone,\n",
    "      },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "    weight_decay=self.weight_decay)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return self.dataloader_train\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return self.dataloader_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the outputs\n",
    "model = Detr(dl_train=train_dataloader, dl_val=val_dataloader, lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lightning_logs\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=48, gradient_clip_val=0.1, accelerator=\"auto\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.push_to_hub(HF_MODEL, private=True)\n",
    "detr_processor.push_to_hub(HF_MODEL, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "id2label = { 0: \"female\", 1: \"not-female\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection.from_pretrained(HF_MODEL, id2label=id2label)\n",
    "processor = DetrImageProcessor.from_pretrained(HF_MODEL)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\n",
    "  [0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "  [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]\n",
    "]\n",
    "\n",
    "def plot_results(pil_img, scores, labels, boxes, id2label):\n",
    "  plt.figure(figsize=(16,10))\n",
    "  plt.imshow(pil_img)\n",
    "  ax = plt.gca()\n",
    "  colors = COLORS * 100\n",
    "  for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n",
    "    text = f'{id2label[label]}: {score:0.2f}'\n",
    "    ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target = test_dataset[0]\n",
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values=pixel_values, pixel_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = target[\"image_id\"].item()\n",
    "image = T.ToPILImage()(pixel_values)\n",
    "\n",
    "width, height = image.size\n",
    "postprocessed_outputs = processor.post_process_object_detection(\n",
    "  outputs,\n",
    "  target_sizes=[(height, width)],\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "results = postprocessed_outputs[0]\n",
    "plot_results(image, results[\"scores\"], results[\"labels\"], results[\"boxes\"], id2label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('hf-model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune DETR to detect female-ish faces in paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade scipy transformers datasets huggingface_hub torch torchvision torchaudio pytorch-lightning pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from hf2coco import create_cocordiais_from_hf\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DetrConfig, DetrForObjectDetection, DetrImageProcessor\n",
    "\n",
    "from PIL import Image as PImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from HF and turn to COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_DATASET = \"thiagohersan/cordiais-faces\"\n",
    "HF_MODEL= \"thiagohersan/detr-cordiais\"\n",
    "COCORDIAIS_PATH = \"./cocordiais\"\n",
    "\n",
    "create_cocordiais_from_hf(HF_DATASET, COCORDIAIS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "  def GaussianNoise(sigma=25.0):\n",
    "    def gauss_noise(img):\n",
    "      dtype = img.dtype\n",
    "      if not img.is_floating_point():\n",
    "        img = img.to(torch.float32)\n",
    "\n",
    "      out = img + sigma * torch.randn_like(img)\n",
    "\n",
    "      if out.dtype != dtype:\n",
    "         out = out.to(dtype)\n",
    "      return out\n",
    "    return gauss_noise\n",
    "\n",
    "  def __init__(self, img_folder, processor, train=True):\n",
    "    ann_file = os.path.join(img_folder, \"cocordiais.json\")\n",
    "    super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "    self.processor = processor\n",
    "    self.train = train\n",
    "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.transform = T.Compose([\n",
    "      T.ColorJitter(brightness=0.5, hue=0.3),\n",
    "      T.ElasticTransform(alpha=30.0),\n",
    "      T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "      T.RandomPosterize(bits=2),\n",
    "      T.RandomEqualize(),\n",
    "      CocoDetection.GaussianNoise(sigma=25.0)\n",
    "    ])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # feel free to add data augmentation here before passing them to the next step\n",
    "    img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "    if self.train:\n",
    "      img = T.PILToTensor()(img).to(self.device)\n",
    "      img = T.ToPILImage()(self.transform(img).to(\"cpu\"))\n",
    "\n",
    "    image_id = self.ids[idx]\n",
    "    target = {\"image_id\": image_id, \"annotations\": target}\n",
    "    encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "    pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "    target = encoding[\"labels\"][0]\n",
    "\n",
    "    return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\n",
    "  \"facebook/detr-resnet-50\",\n",
    "  size={\n",
    "    \"shortest_edge\": 800,\n",
    "    \"longest_edge\": 800\n",
    "  }\n",
    ")\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=os.path.join(COCORDIAIS_PATH, \"train\"), processor=processor)\n",
    "test_dataset = CocoDetection(img_folder=os.path.join(COCORDIAIS_PATH, \"test\"), processor=processor, train=False)\n",
    "\n",
    "print(\"Number of examples:\\n  Train: %s\\n  Test: %s\" % (len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "  batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "  batch[\"labels\"] = labels\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "batch = next(iter(train_dataloader))\n",
    "pixel_values, target = train_dataset[0]\n",
    "\n",
    "print(batch.keys())\n",
    "print(pixel_values.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with PyTorchLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "  def __init__(self, lr, lr_backbone, weight_decay):\n",
    "    super().__init__()\n",
    "    # replace COCO classification head with custom head\n",
    "    self.model = DetrForObjectDetection.from_pretrained(\n",
    "      \"facebook/detr-resnet-50\",\n",
    "      revision=\"no_timm\", \n",
    "      num_labels=len(id2label),\n",
    "      num_queries=16,\n",
    "      ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    self.lr = lr\n",
    "    self.lr_backbone = lr_backbone\n",
    "    self.weight_decay = weight_decay\n",
    "\n",
    "  def forward(self, pixel_values, pixel_mask):\n",
    "    outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "    return outputs\n",
    "\n",
    "  def common_step(self, batch, batch_idx):\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_mask = batch[\"pixel_mask\"]\n",
    "    labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "    outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss_dict = outputs.loss_dict\n",
    "\n",
    "    return loss, loss_dict\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "    self.log(\"training_loss\", loss)\n",
    "    for k,v in loss_dict.items():\n",
    "      self.log(\"train_\" + k, v.item())\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "    self.log(\"validation_loss\", loss)\n",
    "    for k,v in loss_dict.items():\n",
    "      self.log(\"validation_\" + k, v.item())\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    param_dicts = [\n",
    "      {\n",
    "        \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]\n",
    "      },\n",
    "      {\n",
    "        \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": self.lr_backbone,\n",
    "      },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "    weight_decay=self.weight_decay)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return train_dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the outputs\n",
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=32, gradient_clip_val=0.1, accelerator=\"auto\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.push_to_hub(HF_MODEL)\n",
    "processor.push_to_hub(HF_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "id2label = { 0: \"female\", 1: \"not-female\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetrForObjectDetection.from_pretrained(HF_MODEL, id2label=id2label)\n",
    "processor = DetrImageProcessor.from_pretrained(HF_MODEL)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\n",
    "  [0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "  [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]\n",
    "]\n",
    "\n",
    "def plot_results(pil_img, scores, labels, boxes, id2label):\n",
    "  plt.figure(figsize=(16,10))\n",
    "  plt.imshow(pil_img)\n",
    "  ax = plt.gca()\n",
    "  colors = COLORS * 100\n",
    "  for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n",
    "    text = f'{id2label[label]}: {score:0.2f}'\n",
    "    ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target = test_dataset[0]\n",
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values=pixel_values, pixel_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = target[\"image_id\"].item()\n",
    "image = test_dataset.coco.loadImgs(image_id)[0]\n",
    "image = PImage.open(os.path.join(COCORDIAIS_PATH, \"test\", image[\"file_name\"]))\n",
    "\n",
    "width, height = image.size\n",
    "postprocessed_outputs = processor.post_process_object_detection(\n",
    "  outputs,\n",
    "  target_sizes=[(height, width)],\n",
    "  threshold=0.5\n",
    ")\n",
    "\n",
    "results = postprocessed_outputs[0]\n",
    "plot_results(image, results[\"scores\"], results[\"labels\"], results[\"boxes\"], id2label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('hf-model')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
